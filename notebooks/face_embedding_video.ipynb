{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb63943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import uuid\n",
    "from keras.models import load_model\n",
    "from mtcnn import MTCNN\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import skimage\n",
    "import sklearn\n",
    "import pandas as pd  \n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f99613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding import l2_normalize,face_embedding\n",
    "from save_file import save_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a177439",
   "metadata": {},
   "source": [
    "# Process the face detection on a video\n",
    "1. Load the numpy array generated from a video\n",
    "1. Process the face detection\n",
    "1. Visual check of the performances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7036c2",
   "metadata": {},
   "source": [
    "## Load the numpy array : 3D + 3D numpy with colors or 3D if gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f6410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COLOR = True \n",
    "SHOW_DET_FACES = True\n",
    "fps_np = 1.\n",
    "\n",
    "# Load the numpy array \n",
    "path_arrays = \"../data/arrays/\"\n",
    "array_name = 'TH_GNS_' + str(int(fps_np)) + 'fps'\n",
    "#array_name = 'EJ_GNS_' + str(int(fps_np)) + 'fps'\n",
    "\n",
    "if USE_COLOR:\n",
    "    color = '_RGB'\n",
    "else:\n",
    "    color = '_Gray'\n",
    "    \n",
    "# Load the numpy video \n",
    "file_in_np = path_arrays + array_name + color + '.npy'\n",
    "video_np =np.load(file = file_in_np)\n",
    "video_np = video_np.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f59b7d",
   "metadata": {},
   "source": [
    "## Load the DNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454fd9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aro/.local/share/virtualenvs/benchmark-face-reco-f4UNL1EF/lib/python3.9/site-packages/keras/layers/core/lambda_layer.py:297: UserWarning: FaceNet is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  function = cls._parse_function_from_config(config, custom_objects,\n",
      "2022-02-09 23:48:39.685760: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# deep face \n",
    "#model_path = '../data/models/deepface/keras/deepface_fn_512.h5'\n",
    "#model_path = '../data/models/deepface/keras/deepface_fn_128.h5'\n",
    "\n",
    "# face net\n",
    "#model_path = '../data/models/facenet/keras/facenet_ds_keras_128.h5'\n",
    "model_path = '../data/models/facenet/keras/facenet_ds_keras_512.h5'\n",
    "\n",
    "model_emb = load_model(model_path, compile = False , custom_objects={\"tf\": tf})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7330702",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a63d30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save files : embeddings and metadata\n",
    "file_emb_np = path_arrays + array_name + '_emb.npy'\n",
    "file_time = path_arrays + array_name + '_time.npy'\n",
    "file_uuid = path_arrays + array_name + '_uuid.npy'\n",
    "file_out_json = path_arrays + array_name + 'emb.json'\n",
    "\n",
    "\n",
    "\n",
    "# Contour margin for the face cropping\n",
    "margin = 0\n",
    "# mininum size of the face to be\n",
    "min_face_size = 20\n",
    "image_size = 160 # embeddings\n",
    "\n",
    "\n",
    "metadata_video = {\n",
    "                  'video_uri': None,\n",
    "                  'filename_metadata': file_out_json,\n",
    "                  'filename_embeddings': file_emb_np,\n",
    "                  'face_detection' : 'MTCNN',\n",
    "                  'model_emb': 'facenet_ds_keras_512.h5',\n",
    "                  'use_color': USE_COLOR,\n",
    "                  'crop_margin': margin,\n",
    "                  'min_face_size': min_face_size,\n",
    "                  'image_size' : image_size,\n",
    "                  'fps': fps_np}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "587c5c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frame : 153\n",
      "Frame size WxH : 320x180\n",
      "Colors : 3\n"
     ]
    }
   ],
   "source": [
    "# check the size of the video \n",
    "if USE_COLOR:\n",
    "    nb_frame, height, width, color = video_np.shape\n",
    "else:\n",
    "    nb_frame, height, width = video_np.shape\n",
    "    color = 1\n",
    "\n",
    "print('Number of frame : ' + str(nb_frame))\n",
    "print('Frame size WxH : ' + str(width) + 'x' + str(height))\n",
    "print('Colors : ' + str(color))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb35903",
   "metadata": {},
   "source": [
    "## Process the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc1e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "face_detector = MTCNN(min_face_size=min_face_size)\n",
    "\n",
    "model_emb = metadata_video['model_emb']\n",
    "model_path = '../data/models/facenet/keras/' + model_emb\n",
    "model_emb = load_model(model_path, compile=False, custom_objects={\"tf\": tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61adcc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb74f6a160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb74f6a700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# outputs\n",
    "dic_crop_faces = {}\n",
    "video_fd = np.copy(video_np)\n",
    "uuid_faces = []\n",
    "faces_emb = []\n",
    "time = []\n",
    "for iname, image_np in enumerate(video_np):\n",
    "\n",
    "    # for videos : image_np = frame_np\n",
    "    face_crop, face_emb = face_embedding(image_np, face_detector,\n",
    "                                         model_emb,\n",
    "                                         max_nb_faces=10000,\n",
    "                                         margin=margin,\n",
    "                                         image_size=image_size,\n",
    "                                         flag_normalise=False,\n",
    "                                         flag_plot=False)\n",
    "\n",
    "    if len(face_emb) > 0:\n",
    "        uuid_faces.append([uuid.uuid4() for k in range(len(face_emb))])\n",
    "        faces_emb.append(face_emb)\n",
    "\n",
    "        for k in range(np.shape(face_emb)[0]):\n",
    "            time.append(iname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dc1fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_emb_np = np.concatenate(faces_emb, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aed662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(file_emb_np, faces_emb_np, file_out_json, metadata_video)\n",
    "np.save(file=file_time, arr=time)\n",
    "np.save(file=file_uuid, arr=uuid_faces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_reco",
   "language": "python",
   "name": "face_reco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
